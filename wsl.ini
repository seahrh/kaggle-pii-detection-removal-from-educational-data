[DEFAULT]
home_dir=/mnt/c/github/seahrh/kaggle-pii-detection-removal-from-educational-data
input_dir=${home_dir}/input/
output_dir=${home_dir}/output/
pretrained_dir=/mnt/c/huggingface/
tmp_dir=${home_dir}/tmp/
model_dir=${home_dir}/models/
teacher_dir=${home_dir}/teacher/
seed=31

[reduce_lr_on_plateau]
qualified_name=torch.optim.lr_scheduler.ReduceLROnPlateau
min_lr=1e-7
patience=4
factor=0.5
verbose=1

[cosine_annealing_lr]
qualified_name=torch.optim.lr_scheduler.CosineAnnealingLR
T_max=10
verbose=1

[swa_lr]
qualified_name=torch.optim.swa_utils.SWALR
swa_lr=5e-5
anneal_epochs=4
anneal_strategy=cos

[ner]
#job_dir=${model_dir}ner/deberta_v3_base/20240409_053545/
#resume_training_from=${job_dir}lightning_logs/version_0/checkpoints/epoch=1-step=3003-val_loss=0.02742.ckpt
gpus=1 0
train_strategy=ddp
# https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision
train_precision=bf16-mixed
epochs=30
eval_every_n_steps=1000
lr=1e-5
schedulers=reduce_lr_on_plateau
swa_start_epoch=-1
batch_size=16
backbone=deberta_v3_base
model_max_length=512
window_length=512
window_stride=256
patience=20
gradient_checkpointing=0
hidden_dropout_prob=0.1
attention_probs_dropout_prob=0.1
train_data_file=${input_dir}tra_v026.json
train_data_first_n=0
validation_data_file=${input_dir}val_v020.json
dataloader_num_workers=4
ckpt_filename=epoch={epoch}-step={step}-val_loss={val_loss:.5f}
ckpt_save_top_k=3

[mlm]
train_from_scratch=1
train_strategy=auto
#train_strategy=fsdp
# train_precision used only for fsdp or deepspeed, 16-mixed, bf16-mixed
#train_precision=16-mixed
epochs=8
lr=4e-6 4e-6
schedulers=reduce_lr_on_plateau
swa_start_epoch=-1 -1
batch_size=16
backbone=roformer_base
max_position_embeddings=1536
patience=0
gradient_checkpointing=0
hidden_dropout_prob=0.1
attention_probs_dropout_prob=0.1
train_data_file=${input_dir}pretrain_16m.parquet
train_data_sample_frac=1
final_model_validation_percent=5

[deberta_v3_base]
directory=${pretrained_dir}microsoft/deberta-v3-base

[deberta_v3_large]
directory=${pretrained_dir}microsoft/deberta-v3-large

[roformer_base]
directory=${pretrained_dir}roformer-en-base

[longformer_base]
directory=${pretrained_dir}allenai/longformer-base-4096

[longformer_large]
directory=${pretrained_dir}allenai/longformer-large-4096

[llama2_7b]
directory=${pretrained_dir}meta-llama/Llama-2-7b-hf
