[DEFAULT]
gpus=0
home_dir=/home/work/ruhong.seah/
job_dir=
job_ts=
input_dir=${home_dir}clsum/input/
output_dir=${home_dir}clsum/output/
pretrained_dir=${home_dir}pretrained/
tmp_dir=${home_dir}clsum/tmp/
model_dir=${home_dir}clsum/models/
teacher_dir=${home_dir}clsum/teacher/
seed=31

[reduce_lr_on_plateau]
qualified_name=torch.optim.lr_scheduler.ReduceLROnPlateau
min_lr=1e-7
patience=0
factor=0.5
verbose=1

[cosine_annealing_lr]
qualified_name=torch.optim.lr_scheduler.CosineAnnealingLR
T_max=10
verbose=1

[swa_lr]
qualified_name=torch.optim.swa_utils.SWALR
swa_lr=5e-5
anneal_epochs=4
anneal_strategy=cos

[ner]
train_strategy=auto
#train_strategy=fsdp
# train_precision used only for fsdp or deepspeed, 16-mixed, bf16-mixed
#train_precision=16-mixed
epochs=2
lr=2e-5
schedulers=reduce_lr_on_plateau
swa_start_epoch=-1
batch_size=16
backbone=deberta_v3_base
model_max_length=512
patience=2
n_trials=1
n_splits=4
gradient_checkpointing=0
hidden_dropout_prob=0.1
attention_probs_dropout_prob=0.1
train_data_file=${input_dir}train.json
train_data_sample_frac=1
final_model_validation_percent=5

[mlm]
train_from_scratch=1
train_strategy=auto
#train_strategy=fsdp
# train_precision used only for fsdp or deepspeed, 16-mixed, bf16-mixed
#train_precision=16-mixed
epochs=8
lr=4e-6 4e-6
schedulers=reduce_lr_on_plateau
swa_start_epoch=-1 -1
batch_size=16
backbone=roformer_base
max_position_embeddings=1536
patience=0
gradient_checkpointing=0
hidden_dropout_prob=0.1
attention_probs_dropout_prob=0.1
train_data_file=${input_dir}pretrain_16m.parquet
train_data_sample_frac=1
final_model_validation_percent=5

[deberta_v3_base]
directory=${pretrained_dir}microsoft/deberta-v3-base

[deberta_v3_large]
directory=${pretrained_dir}microsoft/deberta-v3-large

[roformer_base]
directory=${pretrained_dir}roformer-en-base

[longformer_base]
directory=${pretrained_dir}allenai/longformer-base-4096

[longformer_large]
directory=${pretrained_dir}allenai/longformer-large-4096

[llama2_7b]
directory=${pretrained_dir}meta-llama/Llama-2-7b-hf
